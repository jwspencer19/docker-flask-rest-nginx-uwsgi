TODO:
- understand deployments and dependencies
- understand networks with deployments, keep together vs anywhere. Pros and cons.
- look into ingress and ability to proxy, can this replace nginx?
- horizontal pod autoscaler (HPA)
- istio
- script Postman
- update JWT access
- logging to elasticsearch
- version deployments and updates, add version to code
- Jenkins for automation
- microservices with a UI. How is the UI split?
- kops to create 3 node cluster (1 master, 2 worker nodes)

Read
https://www.level-up.one/kubernetes-networking-pods-levelup/
https://www.level-up.one/kubernetes-bible-beginners/

# quick way to run a Docker image on Kubernetes cluster without a deployment file
kubectl run my-name-here --image=image-here:version --port 8080

# examples to deploy a Docker image
kubectl run hello-minikube --image=gcr.io/google.containers/echoserver:1.4 --port=8080
kubectl run my-flask-app --image=docker-flask-rest-nginx-uwsgi_flask:latest --port=5000
kubectl run hazelcast --image=hazelcast --port=5701

# expose the deployment, NodePort allows us to have a external IP address exposed
kubectl expose deployment my-flask-app --type=NodePort
kubectl expose deployment hello-minikube --type=NodePort
kubectl expose deployment kubernetes-dashboard --type=NodePort

# useful commmands
kubectl get all --all-namespaces
kubectl describe node
kubectl get pods
kubectl get pods --output=wide
kubectl get pod -o wide --all-namespaces
kubectl describe pod <pod-name>
kubectl exec <pod-name> -- printenv

kubectl cluster-info
kubectl get deployments my-flask-app
kubectl describe deployments my-flask-app
kubectl get services
kubectl describe services my-flask-app
kubectl get all -n kube-system

kubectl create namespace <namespace>
kubectl create -or- apply -f <filename>.yaml --namespace=<namespace>
kubectl describe deployment <deployment-name> --namespace=<namespace>

# delete deployment
kubectl delete deployment my-flask-app

# deploy using a deployment file
kubectl apply -f ./deployment.yaml
# expose it
kubectl expose deployment tomcat-deployment --type=NodePort
# determine its url
kubectl get service

# port forward
kubectl get pod tomcat-deployment
kubectl port-forward tomcat-deployment-846f998dd-896p9 8083:8080

# attach
kubectl attach tomcat-deployment-846f998dd-896p9

# exec
kubectl exec -it tomcat-deployment-846f998dd-896p9 bash
#-or-
kubectl exec -it <pod-name> bash

# label pods using label: healthy=false
kubectl label pods tomcat-deployment-846f998dd-896p9 healthy=false


# scale an existing deployment instead of stopping and editing the deployment.yaml file and restarting
kubectl scale --replicas=4 deployment/tomcat-deployment
# get info on the deployment
kubectl get deployments
kubectl describe deployments tomcat-deployment

# instead of NodePort, we will use LoadBalancer
kubectl expose deployment tomcat-deployment --type=LoadBalancer --port=8080 --target-port=8080 --name tomcat-load-balancer

# see what IP address was assigned for the service
kubectl describe services tomcat-load-balancer

# see the rollout status ofa a deployment
kubectl rollout status deployment tomcat-deployment

# if we wanted to update the tomcat image in our deployment
kubectl set image deployment/tomcat-deployment tomcat=tomcat:9.0.1

# get the rollout history
kubectl rollout history deployment/tomcat-deployment

# get more details on a rollout version
kubectl rollout history deployment/tomcat-deployment --revision=2


# label a node, for example set label storageType=ssd to a node
# first get nodes
kubectl get nodes
kubectl label node bos-spencer-nba-test storageType=ssd

# to see our label
kubectl describe node bos-spencer-nba-test
# note can use nodeSelector in deployment file to choose this node


# health checks: readiness probe and liveness probe in deployment.yaml
cd /home/spencer/Git_hub/kubernetes-demo/Basic and Core Concepts/Health Checks/
# apply the deployment change
kubectl apply -f ./deployment.yaml

# look the details, now see readiness and liveness entries
kubectl describe deployment tomcat-deployment


# Dashboard
microk8s.enable dashboard ingress

# edit kubernetes-dashboard service
# change type: ClusterIP to type: NodePort and save file
kubectl -n kube-system edit service kubernetes-dashboard

# check port on which Dashboard was exposed
kubectl -n kube-system get service kubernetes-dashboard

kubectl describe services kubernetes-dashboard --namespace=kube-system

kubectl get pods --namespace=kube-system
kubectl --namespace=kube-system log kubernetes-dashboard-7d75c474bb-7ph99

kubectl -n kube-system get secret
kubectl -n kube-system describe secret default-token-{xxxx}

# Dashboard alternative?

# install the dashboard - do I need this?
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
# not found, try this instead:
# https://github.com/kubernetes/dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml

# accessing it via a proxy
kubectl proxy
# output:
starting to serve on 127.0.0.1:8001
curl http://localhost:8001/api/

# or
kubectl proxy --port=8080 &
kubectl proxy --port=8082 &

# navigate in browser
http://localhost:8001/ui
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/


# DNS & Service Discovery
C:\Git_repo\kubernetes-demo\Advanced Kubernetes Usage\DNS and Service Discovery

# deploy our MySQL service
kubectl create -f mysql-deployment.yaml

# check if successful
kubectl get pods

# deploy our Wordpress service
kubectl create -f wordpress-deployment.yaml

# check our service and verify it is a LoadBalancer
kubectl get services wordpress

# create file busybox.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always

# create pod using this file
kubectl apply -f busybox.yaml

# test if DNS is working
kubectl exec -ti busybox -- nslookup kubernetes.default

kubectl get pods --namespace=kube-system

# logs on the DNS containers
kubectl logs kube-dns-6bfbdd666c-6rx9w kubedns -n kube-system

# note fix for ubuntu, open the firewall
sudo ufw allow in on cbr0 && sudo ufw allow out on cbr0


# nginx

# first test nginx as a docker container - to debug errors trying to use volume for conf files
docker run --name mynginx2 -v /home/spencer/html:/usr/share/nginx/html -p 8083:80 nginx

# bash into it and tar up conf directory contetns
docker exec -it mynginx2 bash
cd /etc/nginx
tar -cvf /usr/share/nginx/html/files.tar .
# exit docker container
exit
# on host, untar files
cd /home/spencer/mypv
mv /usr/share/nginx/html/files.tar .
tar -xvf files.tar .

# edit config file to flask app
cd /home/spencer/mypv/mynginxdata/conf.d/
vi default.conf
server {
    listen       80;
    server_name  localhost;

    #charset koi8-r;
    #access_log  /var/log/nginx/host.access.log  main;

    location / {
        include uwsgi_params;
        uwsgi_pass flaskapp-svc:8080;
    }

    location /flask {  
        include uwsgi_params;
        uwsgi_pass flaskapp-svc:8080;
    }
...

# run docker container with local copy of config files
docker run --name mynginx2 -v /home/spencer/mypv:/etc/nginx -p 8083:80 nginx
# run in background
docker run --name mynginx2 -v /home/spencer/mypv:/etc/nginx -p 8083:80 -d nginx
# not sure what -P is
docker run --name mynginx2 -v /home/spencer/mypv:/etc/nginx -p 8083:80 -P -d nginx

# exec into nginx pod name running debian
kubectl exec -it nginx-deployment-94d56d68c-rt9p6 bash

# install vi
sudo apt-get update
sudo apt-get install vim

# check nginx status
service nginx status

# restart nginx
service nginx restart


# create a Persistent Volume
kubectl apply -f pv-volume.yaml

# view information about the PersistentVolume:
kubectl get pv task-pv-volume

kubectl describe pv task-pv-volume

# create the PersistentVolumeClaim
kubectl apply -f pv-claim.yaml

# look again at the PersistentVolume, output shows a STATUS of Bound
kubectl get pv task-pv-volume

# look at the  PersistentVolumeClaim:
kubectl get pvc task-pv-claim


Usage & Resource Monitoring
- Heapster is Kubernetes Container Cluster Monitoring solution


Horizontal Pod Autoscaler (HPA)
# example
kubectl autoscale deployment <name> --cpu-percent=50 --min=1 --max=10

# note: might need to start multiple instances to create enough CPU usage
# simulate load using infinite HTTP request loop
kubectl run -i --tty load-generator --image=busybox --generator=run-pod/v1 /bin/sh
(hit enter for command prompt)
while true;do wget -q -O- http://wordpress.default.svc.cluster.local;done


# check the HPA status, might take a while to see the CPU usage to increase
kubectl get hpa


Helm

# install with snap
sudo snap install helm

# init helm to our cluster
helm init

# verify tiller is running
kubectl get pods --namespace kube-system

